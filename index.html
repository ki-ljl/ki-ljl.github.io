<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <!-- DELETE THIS SCRIPT if you use this HTML, otherwise my analytics will track your page -->
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-150103129-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-150103129-1');
  </script>
  
  <meta property="og:type" content="website">
  <meta property="og:url"
    content="vishvakmurahari.com" />
  <meta property="og:title" content="Vishvak Murahari">
  <meta property="og:description" content="Vishvak is a third year PhD student in Computer Science at Princeton University, advised by Dr. Karthik Narasimhan.">
  <meta property="og:image" content="http://www.vishvakmurahari.com/images/profile_headshot.jpg">
  <meta name="twitter:site" value="@VishvakM" />
  <meta property="twitter:title" content="Vishvak Murahari">
  <meta property="twitter:description" content="Vishvak is a third year PhD student in Computer Science at Princeton University, advised by Dr. Karthik Narasimhan.">
  <meta property="twitter:image" content="http://www.vishvakmurahari.com/images/profile_headshot.jpg" >

  <title>Vishvak Murahari</title>
  
  <meta name="author" content="Vishvak Murahari">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Vishvak Murahari</name>
              </p>
              <p>
              I am a computer scientist at 
              at Princeton University, pursuing my Ph.D. advised by <a href="https://www.cs.princeton.edu/~karthikn/">Prof. Karthik Narasimhan</a>. I am interested in Natural Language Processing and Machine Learning.
              </p>
              <p class="content">
              I earned my Bachelors and Masters in Computer Science from Georgia Tech and I was fortunate to be advised by <a href="https://www.cc.gatech.edu/~parikh/vil.html">Prof. Devi Parikh</a> , <a href="https://abhishekdas.com/">Abhishek Das</a> and  <a href="https://www.cc.gatech.edu/people/thomas-ploetz"> Prof. Thomas Ploetz</a>.
              I also worked closely with <a href="https://www.cc.gatech.edu/~dbatra/">Prof. Dhruv Batra</a> and  <a href="http://amanparnami.com/"> Prof. Aman Parnami.</a>. 
              </p>
              <p class="content">I spent some time at <a href="https://research.google/teams/brain/">Google Brain</a> and <a href="https://prior.allenai.org/">Allen AI</a>. I also spent a few summers at <a href="https://www.microsoft.com/en-us/"> Microsoft</a> deploying ML products in Microsoft Office and XBox. </p>
              <p class="content">In my spare time you can catch me reading about Geopolitics and History or find me on the Tennis court. </p>
              <p style="text-align:center">
                <a href="mailto:murahari@princeton.edu">Email</a> &nbsp/&nbsp
                <a href="data/Vishvak_Murahari_Resume.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=Y_NYX7MAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/vmurahari3"> Github </a> &nbsp/&nbsp
                <a href="https://twitter.com/VishvakM"> Twitter </a> &nbsp/&nbsp
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/profile_headshot.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/profile_headshot.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                The problems that I work on lie at the intersection Natural Language Processing, Machine Learning and Computer Vision. Some of my current research interests include: 
              </p>
              <p class="content">
              <ul>
                <li><b><i>Efficient inference of large language models (LLMs) :</i></b> How do we deploy LLMs in a cost-effective manner </li> 
                <li><b><i>Language Pretraining :</i></b> Teaching models to learn good representations from unsupervised data. </li> 
                <li><b><i>Grounded Language Learning:</i></b> Teaching agents to talk about environment specific concepts and entities. </li> 
              </ul>
              </p>
              <p> 
                Representative papers are <span class="highlight">listed under Papers</span>.
              </p>

            </td>
          </tr>
        </tbody></table>
        
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td>
              <heading>Press Converage</heading>
              <ul>
                <li> <a href="https://techcrunch.com/2023/04/12/researchers-discover-a-way-to-make-chatgpt-consistently-toxic/">TechCrunch</a> and <a href="https://venturebeat.com/ai/chatgpt-can-turn-toxic-just-by-changing-its-assigned-persona-researchers-say/"> VentureBeat</a> covered our work which demonstrates fundamental safety concerns in ChatGPT, which is fast becoming an integral part of our technology fabric. </li>
                <li> Our work on data multiplexing in Neural Networks got featured in <a href="https://spectrum.ieee.org/neural-network-multiplex"> IEEE Spectrum.</a></li>
              </ul>
            </td>
          </tr>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td>
              <heading>Achievements</heading>
              <ul>
              	<li> Runners-up for <a href="https://www.bell-labs.com/collaboration-opportunities/prize/bell-labs-prize-2022/"> Bell Labs Innovation Prize, 2022!</a> Awarded 50000 USD in prize money <a href="https://www.cs.princeton.edu/news/small-neural-nets-are-trending-research-shows-density-may-be-answer">[Princeton coverage].</a></li>
              	<li>Selected as a finalist for <a href="https://www.qualcomm.com/research/university-relations/innovation-fellowship/finalists}"> Qualcomm innovation Fellowship, 2022!</a></li>              	
                <li>Awarded the MS Research Award by the College of Computing, Georgia Tech
                <li>Graduated from Georgia Tech Bachelors in Computer Science with Highest Honors.
                <li>Awarded <a href="https://registrar.gatech.edu/info/faculty-honors-letters"> Faculty Honors </a> by Georgia Tech for 5 out of 6 semesters in my undergraduate degree. </li>
                <li>Selected for the prestigious <a href="http://www.ncert.nic.in/programmes/talent_exam/pdf_files/Information_Brochure_2019.pdf">NTSE scholarship offered by the Govt. of India</a> </li>
                <li>Represented India at the <a href="https://wro-association.org/home/">World Robotics Olympiad</a> in 2013 and 2014. Check out our autonomous robots here! <a href="https://www.youtube.com/watch?v=tiU5ZYRZ78Q">[WRO 2014]</a> <a href="https://www.youtube.com/watch?v=yUHsU0vwKco">[WRO 2013]</a></li>
                <li>Awarded first prize in the Indian Robotic Olympiad, 2014 and placed fourth in Indian Robotic Olympiad, 2013.</a>
              </ul>
            </td>
          </tr>
        </table>


        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Papers</heading>
            </td>
          </tr>
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <!-- <heading>Publications</heading> -->
          <tr>
            <td width="30%"><img src="images/csts_teaser.png" alt="3DSP" width="220" height="180" style="border-style: none">
            <td valign="top" width="70%">
              <a href="https://arxiv.org/abs/2305.15093">C-STS: Conditional Semantic Textual Similarity<papertitle></papertitle>
              </a>
            <br>
            Ameet Deshpande,
            Carlos E. Jimenez,
            Howard Chen,
            <strong><u>Vishvak Murahari</u></strong>,
            Victoria Graf,
            Tanmay Rajpurohit,
            Ashwin Kalyan,
            Danqi Chen,
            Karthik Narasimhan
            <br>
              <em>Arxiv preprint</em> 
              <!-- <font color="red">(Poster)</font> -->
              <br>
              <p align="justify">
                Semantic textual similarity (STS) has been a cornerstone task in NLP that measures the degree of similarity between a pair of sentences, with applications in information retrieval, question answering, and embedding methods. However, it is an inherently ambiguous task, with the sentence similarity depending on the specific aspect of interest. We resolve this ambiguity by proposing a novel task called conditional STS (C-STS) which measures similarity conditioned on an aspect elucidated in natural language (hereon, condition). 
                C-STS's advantages are two-fold: (1) it reduces the subjectivity and ambiguity of STS, and (2) enables fine-grained similarity evaluation using diverse conditions. C-STS contains almost 20,000 instances from diverse domains and we demonstrate that even the most performant fine-tuning and in-context learning models (GPT-4, Flan, SimCSE) find it challenging. We encourage the community to evaluate their models on C-STS to provide a more holistic view of semantic similarity and natural language understanding.
              </p> 
            </td>
          </tr>

          <!-- <heading>Publications</heading> -->
          <tr>
            <td width="30%"><img src="images/prumux_teaser.png" alt="3DSP" width="220" height="180" style="border-style: none">
            <td valign="top" width="70%">
              <a href="https://arxiv.org/abs/2305.14706">PruMUX: Augmenting Data Multiplexing with Model Compression<papertitle></papertitle>
              </a>
            <br>
            Yushan Su,
            <strong><u>Vishvak Murahari</u></strong>,
            Kai Li,
            Karthik Narasimhan
            <br>
              <em>ACL Findings 2023</em> 
              <!-- <font color="red">(Poster)</font> -->
              <br>
              <p align="justify"> 
                As language models increase in size by the day, methods for efficient inference are critical to leveraging their capabilities for various applications. Prior work has investigated techniques like model pruning, knowledge distillation, and data multiplexing to increase model throughput without sacrificing accuracy. In this paper, we combine two such methods -- structured pruning and data multiplexing -- to compound the speedup gains obtained by either method. Our approach, PruMUX, obtains up to 7.5-29.5X throughput improvement over BERT-base model with accuracy threshold from 80% to 74%. We further study various combinations of parameters (such as sparsity and multiplexing factor) in the two techniques to provide a comprehensive analysis of the tradeoff between accuracy and throughput in the resulting models. We then propose Auto-PruMUX, a meta-level model that can predict the high-performance parameters for pruning and multiplexing given a desired accuracy loss budget, providing a practical method to leverage the combination effectively.

              </p> 
            </td>
          </tr>

          <tr>
            <td width="30%"><img src="images/toxicity_teaser.jpg" alt="3DSP" width="220" height="150" style="border-style: none">
            <td valign="top" width="70%">
              <a href="https://sites.google.com/princeton.edu/toxicity-in-llms">Toxicity in ChatGPT: Analyzing Persona-assigned Language Models <papertitle></papertitle>
              </a>
            <br>
            Ameet Deshpande^,
            <strong><u> Vishvak Murahari^</u></strong>,
            Tanmay Rajpurohit,
            Ashwin Kalyan,
            Karthik Narasimhan
            <br>
              <em>arxiv preprint</em> 
              <!-- <font color="red">(Poster)</font> -->
              <br>
              <p align="justify"> Large language models (LLMs) have shown incredible capabilities and transcended the natural language processing (NLP) community and the safety of these systems is of prime importance. 
                To this end, we systematically evaluate toxicity in over half a million generations of ChatGPT, a popular dialogue-based LLM. We find that setting the system parameter of ChatGPT by assigning it a persona, say that of the boxer Muhammad Ali, significantly increases the toxicity of generations. 
                Depending on the persona assigned to ChatGPT, its toxicity can increase up to 6x, with outputs engaging in incorrect stereotypes, harmful dialogue, and hurtful opinions. 
                Furthermore, we find concerning patterns where specific entities (e.g., certain races) are targeted more than others (3x more) irrespective of the assigned persona, that reflect inherent discriminatory biases in the model. 
                We hope that our findings inspire the broader AI community to rethink the efficacy of current safety guardrails and develop better techniques that lead to robust and trustworthy AI systems.
              </p> 
            </td>
          </tr>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <!-- <heading>Publications</heading> -->
          <tr>
            <td width="30%"><img src="images/mux_plm.gif" alt="3DSP" width="220" height="150" style="border-style: none">
            <td valign="top" width="70%">
              <a href="https://arxiv.org/abs/2302.12441">MUX-PLMs: Pre-training Language Models with Data Multiplexing <papertitle></papertitle>
              </a>
            <br>
            <strong><u> Vishvak Murahari </u></strong>,
            Ameet Deshpande,
            Carlos E. Jimenez,
            Izhak Shafran,
            Mingqiu Wang,
            Yuan Cao,
            Karthik Narasimhan
            <br>
              <em>Arxiv preprint</em> 
              <!-- <font color="red">(Poster)</font> -->
              <br>
              <a href="https://github.com/princeton-nlp/datamux-pretraining"> [Code]</a>
              <p align="justify"> 
                The widespread adoption of large language models such as ChatGPT and Bard has led to unprecedented demand for these technologies. The burgeoning cost of inference for ever-increasing model sizes coupled with hardware shortages has limited affordable access and poses a pressing need for efficiency approaches geared towards high throughput and performance. Multi-input multi-output (MIMO) algorithms such as data multiplexing, offer a promising solution with a many-fold increase in throughput by performing inference for multiple inputs at the cost of a single input. Yet these approaches are not currently performant enough to be deployed in modern systems. We change that by developing MUX-PLMs, a class of high throughput pre-trained language models (PLMs) trained with data multiplexing, that can be fine-tuned for any downstream task to yield high-throughput high-performance. Our novel multiplexing and demultiplexing modules proficiently entangle and disentangle inputs, and enable high-performance high throughput MUX-PLMs that are competitive with vanilla PLMs while achieving 2x/5x inference speedup with only a 1-4 % drop on a broad suite of tasks.

              </p> 
            </td>
          </tr>

          <tr>
          <td width="30%"><img src="images/asap.gif" alt="3DSP" width="220" height="150" style="border-style: none">
          <td valign="top" width="70%">
                  <a href="https://asap-benchmark.github.io/">
                      <papertitle>Building Scalable Video Understanding Benchmarks through Sports </papertitle>
                  </a>
                  <br>
                  Aniket Agarwal^, Alex Zhang^, Karthik Narasimhan, Igor Gilitschenski, <br> <strong><u>Vishvak Murahari</u>*</strong>, Yash Kant*
                  <br>
                  <a href="https://arxiv.org/abs/2301.06866">[Paper]</a> /
                  <a href="https://asap-benchmark.github.io/">[Webpage]</a> /
                  <a href="https://github.com/asap-benchmark/">[Code]</a> 
                  <p align="justify">We introduce an automated Annotation and Video Stream Alignment Pipeline (abbreviated ASAP) for aligning unlabeled videos of four different sports (Cricket, Football, Basketball, and American Football) with their corresponding dense annotations (commentary) freely available on the web. Our human studies indicate that ASAP can align videos and annotations with high fidelity, precision, and speed!</p> 
              </td>
          </tr>

          <tr>
            <td width="30%"><img src="images/transformer_schematic.jpg" alt="3DSP" width="220" height="150" style="border-style: none">
            <td valign="top" width="70%">
              <a href="https://arxiv.org/abs/2202.09318">DataMUX: Data Multiplexing for Neural Networks<papertitle></papertitle>
              </a>
            <br>
            <strong><u> Vishvak Murahari </u></strong>,
            Carlos E. Jimenez,
            Runzhe Yang,
            Karthik Narasimhan
            <br>
              <em>NeurIPS 2022</em> 
              <!-- <font color="red">(Poster)</font> -->
              <br>
              <a href="https://github.com/princeton-nlp/DataMUX"> [Code]</a>
              <a href="https://princeton-nlp.github.io/DataMUX/"> [Webpage]</a>
              <p align="justify"> We introduce data multiplexing (DataMUX), a technique that enables deep neural networks to process multiple inputs simultaneously using a single compact representation. DataMUX demonstrates that neural networks are capable of generating accurate predictions over mixtures of inputs, resulting in increased throughput with minimal extra memory requirements. Our approach uses two key components -- 1) a multiplexing layer that performs a fixed linear transformation to each input before combining them to create a mixed representation of the same size as a single input, which is then processed by the base network, and 2) a demultiplexing layer that converts the base network's output back into independent representations before producing predictions for each input. We show the viability of DataMUX for different architectures (Transformers, and to a lesser extent MLPs and CNNs) across six different tasks spanning sentence classification, named entity recognition and image classification. For instance, DataMUX for Transformers can multiplex up to 20x/40x inputs, achieving 11x/18x increase in throughput with minimal absolute performance drops of 2% and 4% respectively on MNLI, a natural language inference task.</p> 
            </td>
          </tr>

          <tr>
            <td width="30%"><img src="images/visdial_bert_viz.png" alt="3DSP" width="220" height="150" style="border-style: none">
            <td valign="top" width="70%">
              <a href="https://arxiv.org/abs/1912.02379">Large-scale Pretraining for Visual Dialog: A Simple State-of-the-Art Baseline <papertitle></papertitle>
              </a>
            <br>
            <strong><u> Vishvak Murahari </u></strong>,
            Dhruv Batra,
            Devi Parikh,
            Abhishek Das
            <br>
              <em>ECCV 2020</em> 
              <!-- <font color="red">(Poster)</font> -->
              <br>
              <a href="https://github.com/vmurahari3/visdial-bert"> [Code]</a>
              <a href="https://drive.google.com/file/d/1MMpnEslS_HKzVD79kGx2dBrBo4Ecs_AE/view?usp=sharing"> [Talk]</a>
              <p align="justify">Prior work in visual dialog has focused on training deep neural models on VisDial in isolation. Instead, we present an approach to leverage pretraining on related vision-language datasets before transferring to visual dialog. Our best single model outperforms prior published work (including model ensembles) by more than 1% absolute on NDCG and MRR. Next, we find that additional finetuning using "dense" annotations in VisDial leads to even higher NDCG -- more than 10% over our base model -- but hurts MRR -- more than 17% below our base model! This highlights a trade-off between the two primary metrics -- NDCG and MRR -- which we find is due to dense annotations not correlating well with the original ground-truth answers to questions.</p> 
            </td>
          </tr>

          <tr>
            <td width="30%"><img src="images/visdial_div_teaser.jpg" alt="3DSP" width="220" height="150" style="border-style: none">
            <td valign="top" width="70%">
              <a href="https://arxiv.org/abs/1909.10470">
                      <papertitle>Improving Generative Visual Dialog by Answering Diverse Questions</papertitle>
              </a>
            <br>
            <strong><u> Vishvak Murahari </u></strong>,
            Prithvijit Chattopadhyay,
            Dhruv Batra,
            Devi Parikh,
            Abhishek Das
            <br>
              <em>EMNLP</em>, 2019 
              <!-- <font color="red">(Poster)</font> -->
              <br>
              <a href="https://github.com/vmurahari3/visdial-diversity"> [Code]</a>
              <a href="https://drive.google.com/open?id=11Xevs0IbMV0aTwdwW9wUlbKCP3GVe8r3"> [Poster]</a>
              <p align="justify">While generative visual dialog models trained with self-talk based RL perform better at the associated downstream task, they suffer from repeated interactions -- resulting in saturation in improvements as the number of rounds increase. To counter this, we devise a simple auxiliary objective that incentivizes Q-Bot to ask diverse questions, thus reducing repetitions and in turn enabling A-Bot to explore a larger state space during RL i.e., be exposed to more visual concepts to talk about, and varied questions to answer.</p> 
            </td>
          </tr>


          <tr>
            <td width="30%"><img src="images/iswc_teaser.jpg" alt="iswc2018" width="200" height="71" style="border-style: none">
            <td valign="top" width="70%">
              <a href="https://dl.acm.org/citation.cfm?id=3267287">
                      <papertitle> On attention models in human activity recognition
</papertitle>
              </a>
            <br>
            <strong><u>Vishvak Murahari</u></strong>,
            Thomas Ploetz
            <br>
              <a href="http://iswc.net/iswc20/"><em>ISWC</em> 2018</a> <br>
              <p align="justify">Most approaches that model time-series data in human activity recognition based on body-worn sensing (HAR) use a fixed
                                size temporal context to represent different activities. This might, however, not be apt for sets of activities with individually varying durations. We introduce attention models into HAR research as a data driven approach for exploring relevant temporal context. Attention models learn a set of weights over input data, which we leverage to weight the temporal context being considered to model each sensor reading. We also visualize the learned weights to better understand what constitutes relevant temporal context.</p> 
            </td>
          </tr>

        </tbody></table>


        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Teaching</heading>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <!-- <heading>Publications</heading> -->
          <tr>
            <td width="30%"><img src="images/cos324.jpeg" alt="3DSP" width="220" height="150" style="border-style: none">
            <td valign="top" width="70%">
              
                      <papertitle>Teaching Assistant, Introduction to Machine Learning (COS 324)</papertitle>
            
            <br>
              <!-- <font color="red">(Poster)</font> -->

              <p align="justify">As a TA for COS 324, I was a part of one of the largest classes at Princeton, taken by close to 200 students. I advised students on topics ranging from Optimization, Neural Networks and Reinforcement Learning. I collaborated with co-TAs to develop assignments and I also engaged with students in-person through weekly recitations and office hours.</p> 
            </td>
          </tr>

          <tr>
            <td width="30%"><img src="images/cozmo.jpeg" alt="3DSP" width="220" height="150" style="border-style: none">
            <td valign="top" width="70%">
              
                      <papertitle>Teaching Assistant, Introduction to Robotics and Perception (CS 3630)</papertitle>
            
            <br>
              <!-- <font color="red">(Poster)</font> -->

              <p align="justify">As a TA for CS 3630, I was a part of one of the largest hands-on advanced robotics classes in the country, taken by close to 200 students. I advised students on robotic planning, control and localization. I collaborated with co-TAs to develop and improve projects on robot localization. I also engaged with students in-person through weekly office hours and also engaged through Piazza.</p> 
            </td>
          </tr>

          <tr></tr>
          <tr>
            <td width="30%"><img src="images/pacman.png" alt="iswc2018" width="200" height="100" style="border-style: none">
            <td valign="top" width="70%">
                      <papertitle> Teaching Assistant, Introduction to AI (CS 3600)</papertitle>
              <p align="justify"> Guided more than 300 students on AI projects and homework. Reinforced concepts ranging from probabilistic inference to Neural Networks, Optimization and Reinforcement Learning. Helped in course development and helped improve existing class projects. Held weekly office hours to engage with students.</p> 
            </td>
          </tr>

        </tbody></table>
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
                  <hr>
                  <p align="center">
                  <font>(Design and CSS courtesy: <a href="https://jonbarron.info/">Jon Barron</a> and <a href="https://abhoi.github.io/">Amlaan Bhoi</a>)</font>
                  </p>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
