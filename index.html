<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <!-- DELETE THIS SCRIPT if you use this HTML, otherwise my analytics will track your page -->
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-150103129-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-150103129-1');
  </script>
  
  <meta property="og:type" content="website">
  <meta property="og:url"
    content="vishvakmurahari.com" />
  <meta property="og:title" content="Vishvak Murahari">
  <meta property="og:description" content="Vishvak is a second year PhD student in Computer Science at Princeton University, advised by Dr. Karthik Narasimhan.">
  <meta property="og:image" content="http://www.vishvakmurahari.com/images/profile_headshot.jpg">
  <meta name="twitter:site" value="@VishvakM" />
  <meta property="twitter:title" content="Vishvak Murahari">
  <meta property="twitter:description" content="Vishvak is a second year PhD student in Computer Science at Princeton University, advised by Dr. Karthik Narasimhan.">
  <meta property="twitter:image" content="http://www.vishvakmurahari.com/images/profile_headshot.jpg" >

  <title>Vishvak Murahari</title>
  
  <meta name="author" content="Vishvak Murahari">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Vishvak Murahari</name>
              </p>
              <p>
              I am a 2<sup>nd</sup> year CS PhD 
              student (focus on Natural Language Processing, Machine Learning) at Princeton University, advised by <a href="https://www.cs.princeton.edu/~karthikn/">Prof. Karthik Narsimhan</a>. I am also a Student Researcher at Google Princeton, advised by <a href="https://www.ehazan.com/">Prof. Elad Hazan</a>
              </p>
              <p class="content">
              I earned my Masters from Georgia Tech and I was fortunate to be advised by <a href="https://www.cc.gatech.edu/~parikh/vil.html">Prof. Devi Parikh</a> and <a href="https://abhishekdas.com/">Abhishek Das</a>. I also worked closely with <a href="https://www.cc.gatech.edu/~dbatra/">Prof. Dhruv Batra</a>. I earned my Bachelors in Computer Science (focus on AI and Devices) from Georgia Tech and I was fortunate to be advised by <a href="https://www.cc.gatech.edu/people/thomas-ploetz"> Prof. Thomas Ploetz</a> and worked closely with <a href="http://amanparnami.com/"> Prof. Aman Parnami.</a>
              </p>
              <p class="content">I previously interned with the <a href="https://prior.allenai.org/">PRIOR</a> team at AI2 (Summer 2020) and was advised by <a href="https://roozbehm.info/">Roozbeh Mottaghi</a> and worked on some interesting problems in Instruction following. I have also had the fortune to intern at <a href="https://www.microsoft.com/en-us/"> Microsoft, Redmond</a> (Summer 2019, 2018, 2017) where I have worked on improving the query re-formulation algorithms at Outlook 365, designing recommendation systems for XBox and developing low latency systems to back large scale privacy dashboard for Windows 10 Users.</p>
              <p class="content">In my spare time you can catch me reading about Geopolitics and History or find me on the Tennis court. </p>
              <p style="text-align:center">
                <a href="mailto:murahari@princeton.edu">Email</a> &nbsp/&nbsp
                <a href="data/Vishvak_Murahari_Resume.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=Y_NYX7MAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/vmurahari3"> Github </a> &nbsp/&nbsp
                <a href="https://twitter.com/VishvakM"> Twitter </a> &nbsp/&nbsp
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/profile_headshot.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/profile_headshot.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                The problems that I work on lie at the intersection Natural Language Processing, Machine Learning and Computer Vision. Some of my current research interests include: 
              </p>
              <p class="content">
              <ul>
                <li><b><i>Language Pretraining/ RL pretraining :</i></b> Teaching agents to learn good representations from unsupervised data. </li> 
                <li><b><i>Grounded Language Learning:</i></b> Teaching agents to talk about environment specific concepts and entities. </li> 
                <li><b><i>Learning language through interaction:</i></b> Teaching agents to talk through either self-play or by interacting with language based environments.</li>

              </ul>
              </p>
              <p> 
                Representative papers are <span class="highlight">listed under Papers</span>.
              </p>

            </td>
          </tr>
        </tbody></table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td>
              <heading>Achievements</heading>
              <ul>
                <li>Awarded the MS Research Award by the College of Computing, Georgia Tech
                <li>Graduated from Georgia Tech Bachelors in Computer Science with Highest Honors.
                <li>Awarded <a href="https://registrar.gatech.edu/info/faculty-honors-letters"> Faculty Honors </a> by Georgia Tech for 5 out of 6 semesters in my undergraduate degree. </li>
                <li>Selected for the prestigious <a href="http://www.ncert.nic.in/programmes/talent_exam/pdf_files/Information_Brochure_2019.pdf">NTSE scholarship offered by the Govt. of India</a> </li>
                <li>Represented India at the <a href="https://wro-association.org/home/">World Robotics Olympiad</a> in 2013 and 2014. Check out our autonomous robots here! <a href="https://www.youtube.com/watch?v=tiU5ZYRZ78Q">[WRO 2014]</a> <a href="https://www.youtube.com/watch?v=yUHsU0vwKco">[WRO 2013]</a></li>
                <li>Awarded first prize in the Indian Robotic Olympiad, 2014 and placed fourth in Indian Robotic Olympiad, 2013.</a>
              </ul>
            </td>
          </tr>
        </table>


        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Papers</heading>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <!-- <heading>Publications</heading> -->

          <tr>
            <td width="30%"><img src="images/transformer_schematic.jpg" alt="3DSP" width="220" height="150" style="border-style: none">
            <td valign="top" width="70%">
              <a href="https://arxiv.org/abs/2202.09318">DataMUX: Data Multiplexing for Neural Networks<papertitle></papertitle>
              </a>
            <br>
            <strong><u> Vishvak Murahari </u></strong>,
            Carlos E. Jimenez,
            Runzhe Yang,
            Karthik Narasimhan
            <br>
              <em>arxiv preprint</em> 
              <!-- <font color="red">(Poster)</font> -->
              <br>
              <a href="https://github.com/princeton-nlp/DataMUX"> [Code]</a>
              <a href="https://princeton-nlp.github.io/DataMUX/"> [Webpage]</a>
              <p align="justify"> We introduce data multiplexing (DataMUX), a technique that enables deep neural networks to process multiple inputs simultaneously using a single compact representation. DataMUX demonstrates that neural networks are capable of generating accurate predictions over mixtures of inputs, resulting in increased throughput with minimal extra memory requirements. Our approach uses two key components -- 1) a multiplexing layer that performs a fixed linear transformation to each input before combining them to create a mixed representation of the same size as a single input, which is then processed by the base network, and 2) a demultiplexing layer that converts the base network's output back into independent representations before producing predictions for each input. We show the viability of DataMUX for different architectures (Transformers, and to a lesser extent MLPs and CNNs) across six different tasks spanning sentence classification, named entity recognition and image classification. For instance, DataMUX for Transformers can multiplex up to 20x/40x inputs, achieving 11x/18x increase in throughput with minimal absolute performance drops of 2% and 4% respectively on MNLI, a natural language inference task.</p> 
            </td>
          </tr>

          <tr>
            <td width="30%"><img src="images/visdial_bert_viz.png" alt="3DSP" width="220" height="150" style="border-style: none">
            <td valign="top" width="70%">
              <a href="https://arxiv.org/abs/1912.02379">Large-scale Pretraining for Visual Dialog: A Simple State-of-the-Art Baseline <papertitle></papertitle>
              </a>
            <br>
            <strong><u> Vishvak Murahari </u></strong>,
            Dhruv Batra,
            Devi Parikh,
            Abhishek Das
            <br>
              <em>ECCV 2020</em> 
              <!-- <font color="red">(Poster)</font> -->
              <br>
              <a href="https://github.com/vmurahari3/visdial-bert"> [Code]</a>

              <p align="justify">Prior work in visual dialog has focused on training deep neural models on VisDial in isolation. Instead, we present an approach to leverage pretraining on related vision-language datasets before transferring to visual dialog. Our best single model outperforms prior published work (including model ensembles) by more than 1% absolute on NDCG and MRR. Next, we find that additional finetuning using "dense" annotations in VisDial leads to even higher NDCG -- more than 10% over our base model -- but hurts MRR -- more than 17% below our base model! This highlights a trade-off between the two primary metrics -- NDCG and MRR -- which we find is due to dense annotations not correlating well with the original ground-truth answers to questions.</p> 
            </td>
          </tr>

          <tr>
            <td width="30%"><img src="images/visdial_div_teaser.jpg" alt="3DSP" width="220" height="150" style="border-style: none">
            <td valign="top" width="70%">
              <a href="https://arxiv.org/abs/1909.10470">
                      <papertitle>Improving Generative Visual Dialog by Answering Diverse Questions</papertitle>
              </a>
            <br>
            <strong><u> Vishvak Murahari </u></strong>,
            Prithvijit Chattopadhyay,
            Dhruv Batra,
            Devi Parikh,
            Abhishek Das
            <br>
              <em>EMNLP</em>, 2019 
              <!-- <font color="red">(Poster)</font> -->
              <br>
              <a href="https://github.com/vmurahari3/visdial-diversity"> [Code]</a>

              <p align="justify">While generative visual dialog models trained with self-talk based RL perform better at the associated downstream task, they suffer from repeated interactions -- resulting in saturation in improvements as the number of rounds increase. To counter this, we devise a simple auxiliary objective that incentivizes Q-Bot to ask diverse questions, thus reducing repetitions and in turn enabling A-Bot to explore a larger state space during RL i.e., be exposed to more visual concepts to talk about, and varied questions to answer.</p> 
            </td>
          </tr>


          <tr>
            <td width="30%"><img src="images/iswc_teaser.jpg" alt="iswc2018" width="200" height="71" style="border-style: none">
            <td valign="top" width="70%">
              <a href="https://dl.acm.org/citation.cfm?id=3267287">
                      <papertitle> On attention models in human activity recognition
</papertitle>
              </a>
            <br>
            <strong><u>Vishvak Murahari</u></strong>,
            Thomas Ploetz
            <br>
              <a href="http://iswc.net/iswc20/"><em>ISWC</em> 2018</a> <br>
              <p align="justify">Most approaches that model time-series data in human activity recognition based on body-worn sensing (HAR) use a fixed
                                size temporal context to represent different activities. This might, however, not be apt for sets of activities with individually varying durations. We introduce attention models into HAR research as a data driven approach for exploring relevant temporal context. Attention models learn a set of weights over input data, which we leverage to weight the temporal context being considered to model each sensor reading. We also visualize the learned weights to better understand what constitutes relevant temporal context</p> 
            </td>
          </tr>

        </tbody></table>


        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Teaching</heading>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <!-- <heading>Publications</heading> -->
          <tr>
            <td width="30%"><img src="images/cozmo.jpeg" alt="3DSP" width="220" height="150" style="border-style: none">
            <td valign="top" width="70%">
              
                      <papertitle>Teaching Assistant, Introduction to Robotics and Perception (CS 3630)</papertitle>
            
            <br>
              <!-- <font color="red">(Poster)</font> -->

              <p align="justify">As a TA for CS 3630, I was a part of one of the largest hands-on advanced robotics classes in the country, taken by close to 200 students. I advised students on robotic planning, control and localization. I collaborated with co-TAs to develop and improve 2 projects on robot localization. I also engaged with students in-person through weekly office hours and also engaged online through Piazza</p> 
            </td>
          </tr>

          <tr></tr>
          <tr>
            <td width="30%"><img src="images/pacman.png" alt="iswc2018" width="200" height="100" style="border-style: none">
            <td valign="top" width="70%">
                      <papertitle> Teaching Assistant, Introduction to AI (CS 3600)</papertitle>
              <p align="justify"> Guided more than 300 students on AI projects and homework. Reinforced concepts ranging from probabilistic inference to Neural Networks, Optimization and Reinforcement Leaning. Helped in course development and helped improve existing class projects. Held weekly office hours to engage with students</p> 
            </td>
          </tr>

        </tbody></table>
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
                  <hr>
                  <p align="center">
                  <font>(Design and CSS courtesy: <a href="https://jonbarron.info/">Jon Barron</a> and <a href="https://abhoi.github.io/">Amlaan Bhoi</a>)</font>
                  </p>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
